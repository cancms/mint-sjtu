<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=generator content="Hugo Blox Builder 5.9.7"><link rel=stylesheet href=/mint-sjtu/css/vendor-bundle.min.26c458e6907dc03073573976b7f4044e.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1.9.4/css/academicons.min.css integrity="sha512-IW0nhlW5MgNydsXJO40En2EoCkTTjZhI3yuODrZIc8cQ4h1XcF53PsqDHa09NqnkXuIe0Oiyyj171BqZFwISBw==" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.css integrity crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=/mint-sjtu/css/wowchemy.08c2b45e508bfe62e141a190395f1502.css><link rel=stylesheet href=/mint-sjtu/css/libs/chroma/github-light.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=/mint-sjtu/css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><meta name=description content="A highly-customizable Hugo research group theme powered by Wowchemy website builder."><link rel=alternate hreflang=en-us href=https://cancms.github.io/mint-sjtu/><link rel=canonical href=https://cancms.github.io/mint-sjtu/><link rel=manifest href=/mint-sjtu/manifest.webmanifest><link rel=icon type=image/png href=/mint-sjtu/media/icon_hu_2311fe1a408f47c1.png><link rel=apple-touch-icon type=image/png href=/mint-sjtu/media/icon_hu_4eb7679362359773.png><meta name=theme-color content="#1565c0"><meta property="twitter:card" content="summary"><meta property="twitter:site" content="@GetResearchDev"><meta property="twitter:creator" content="@GetResearchDev"><meta property="twitter:image" content="https://cancms.github.io/mint-sjtu/media/logo_hu_479bc4b5e23128ef.png"><meta property="og:type" content="website"><meta property="og:site_name" content="MINT-SJTU"><meta property="og:url" content="https://cancms.github.io/mint-sjtu/"><meta property="og:title" content="MINT-SJTU"><meta property="og:description" content="A highly-customizable Hugo research group theme powered by Wowchemy website builder."><meta property="og:image" content="https://cancms.github.io/mint-sjtu/media/logo_hu_479bc4b5e23128ef.png"><meta property="og:locale" content="en-us"><meta property="og:updated_time" content="2025-04-28T00:00:00+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"WebSite","potentialAction":{"@type":"SearchAction","target":"https://cancms.github.io/mint-sjtu/?q={search_term_string}","query-input":"required name=search_term_string"},"url":"https://cancms.github.io/mint-sjtu/"}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","@id":"https://cancms.github.io/mint-sjtu/","name":"MINT-SJTU","logo":"https://cancms.github.io/mint-sjtu/media/logo_hu_fa60714a01738c90.png","url":"https://cancms.github.io/mint-sjtu/"}</script><script src=https://identity.netlify.com/v1/netlify-identity-widget.js></script><link rel=alternate href=/mint-sjtu/index.xml type=application/rss+xml title=MINT-SJTU><title>MINT-SJTU</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=3976528693a0108357f4928017600865><script src=/mint-sjtu/js/wowchemy-init.min.4fef3e534144e9903491f0cc6527eccd.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class="page-header header--fixed"><header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/mint-sjtu/><img src=/mint-sjtu/media/logo_hu_9db479d7212a6aac.png alt=MINT-SJTU></a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/mint-sjtu/><img src=/mint-sjtu/media/logo_hu_9db479d7212a6aac.png alt=MINT-SJTU></a></div><div class="navbar-collapse main-menu-item collapse justify-content-end" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/mint-sjtu/ data-target=[]><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/mint-sjtu/recruiting><span>Recruiting</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"></ul></div></nav></header></div><div class=page-body><section id=section-hero class="home-section wg-hero"><div class=container><div class=row><div class="col-12 hero-title">Machine Intelligence and Interaction Lab @ SJTU<br><br>上海交通大学 机器智能与交互实验室</div></div><div class=row><div class="col-12 hero-desc-txt"><p>Bo Zhao is an Associate Professor (Tenure Track) at School of Artificial Intelligence, Shanghai Jiao Tong University. Before, he was with BAAI as Principal Investigator, leading DCAI group. He received Ph.D. from The University of Edinburgh and M.Eng. from Peking University. His research interests include Embodied AI, Multimodal LLM and Data-centric AI. He received ICML 2022 Outstanding Paper Award. He was the only nominee of The University of Edinburgh for Informatics-Europe Best Dissertation Award 2023. He received NSFC fundings on MLLMs and Dataset Condensation. He served as an Area Chair for NeurIPS'25/24 and BMVC'24, and organizers for DD workshops at CVPR'24 and ECCV'24.</p><p>赵波是上海交通大学人工智能学院长聘教轨副教授、博导，入选国家级青年人才项目。曾担任智源研究院（BAAI）数据智能研究中心负责人、首席研究员。曾获得爱丁堡大学博士学位和北京大学硕士学位。研究方向包括具身智能、多模态大模型和数据智能（DCAI）等。曾获 ICML 2022 杰出论文奖，并作为爱丁堡大学唯一提名人入围2023年欧洲信息学最佳博士论文奖候选名单。主持多项国自然基金委科研项目。担任 NeurIPS'25/24和BMVC'24 领域主席，并于 CVPR'24 和 ECCV'24 组织数据集蒸馏研讨会。</p><p>I am working on Embodied AI, MLLM and Data-centric AI. Collaborations are welcome. Feel free to contact me. I am recruiting Ph.D./Master Students and Research Assistants/Interns. If you are interested, please read the Recruiting page (top-right).</p><p>实验室研究方向：具身智能，多模态大模型，数据智能等。实验室常年招收硕博士生以及实习生，详情请阅读右上角招聘页面。</p></div></div></div></section><section id=section-hero class="home-section wg-hero"><div class=container><div class=row><div class="col-12 hero-media carousel slide" data-ride=carousel id=hero_sliders><ol class=carousel-indicators><li data-target=#hero_sliders data-slide-to=0 class=active></li><li data-target=#hero_sliders data-slide-to=1></li></ol><div class=carousel-inner><div class="carousel-item active" style=height:415px;background-color:#666;background-image:url(https://cancms.github.io/mint-sjtu/media/group_slides/s1.webp);background-repeat:no-repeat;background-position:50%;background-size:cover><div class="position-absolute d-flex w-100 h-100 justify-content-center align-items-center" style=-webkit-backdrop-filter:brightness(1);backdrop-filter:brightness(1)><div class="wg-hero dark container" style=margin-left:6rem;margin-right:6rem;text-align:center><h1 class=hero-title>desc-title1</h1><p class=hero-lead style="margin:0 auto">desc1</p></div></div></div><div class=carousel-item style=height:415px;background-color:#666;background-image:url(https://cancms.github.io/mint-sjtu/media/group_slides/s2.webp);background-repeat:no-repeat;background-position:50%;background-size:cover><div class="position-absolute d-flex w-100 h-100 justify-content-center align-items-center" style=-webkit-backdrop-filter:brightness(1);backdrop-filter:brightness(1)><div class="wg-hero dark container" style=margin-left:6rem;margin-right:6rem;text-align:left><h1 class=hero-title>desc-title2</h1><p class=hero-lead>desc2</p></div></div></div></div><a class=carousel-control-prev href=#hero_sliders data-slide=prev><span class=carousel-control-prev-icon></span>
<span class=sr-only>Previous</span>
</a><a class=carousel-control-next href=#hero_sliders data-slide=next><span class=carousel-control-next-icon></span>
<span class=sr-only>Next</span></a></div></div></div></section><section id=section-recent-works class=home-section><div class=container><div class=section-c-title>Recent Works</div><div class="grid grid-cols-1 gap-x-12 gap-y-16 lg:grid-cols-3"><div><div class=card><a href=https://mint-sjtu.github.io/STI-Bench.io/><img src=/mint-sjtu/media/recent_works/rw1.jpg alt></a><div class=card-body><p class=card-text>STI-Bench: a benchmark designed to evaluate MLLMs’ spatial-temporal understanding through challenging tasks such as estimating and predicting the appearance, pose, displacement, and motion of objects.</p></div></div></div><div><div class=card><a href=https://github.com/VectorSpaceLab/Video-XL/tree/main><img src=/mint-sjtu/media/recent_works/rw2.jpg alt></a><div class=card-body><p class=card-text>[CVPR'25 Oral] We propose Video-XL, a novel approach that leverages MLLMs’ inherent KV sparsification capacity to condense the visual input realizes outstanding cost-effectiveness, enabling high-quality processing of thousands of frames on a single A100 GPU.</p></div></div></div><div><div class=card><a href=https://github.com/BAAI-DCAI/SpatialBot/><img src=/mint-sjtu/media/recent_works/rw3.jpg alt></a><div class=card-body><p class=card-text>[ICRA'25] We propose SpatialBot, a family of state-of-the-art VLMs, for effective depth understanding and thus precise robot manipulating in embodied AI by training on our constructed SpatialQA and SpatialQA-E datasets.</p></div></div></div><div><div class=card><a href=https://jiyao06.github.io/Omni6DPose/><img src=/mint-sjtu/media/recent_works/rw4.jpg alt></a><div class=card-body><p class=card-text>[ECCV'24] We introduces Omni6DPose, a substantial benchmark featured by its diversity in object categories, large scale, and variety in object materials, across 581 instances in 149 categories.</p></div></div></div><div><div class=card><a href=https://github.com/BAAI-DCAI/SegVol><img src=/mint-sjtu/media/recent_works/rw5.jpg alt></a><div class=card-body><p class=card-text>[NeurIPS'24 Spotlight] We propose a 3D foundation segmentation model, named SegVol, supporting universal and interactive volumetric medical image segmentation, supporting the segmentation of over 200 anatomical categories.</p></div></div></div><div><div class=card><a href=https://emu.baai.ac.cn/about><img src=/mint-sjtu/media/recent_works/rw6.jpg alt></a><div class=card-body><p class=card-text>We introduce Emu3, a new suite of state-of-the-art multimodal models trained solely with next-token prediction. By tokenizing images, text, and videos into a discrete space, we train a single transformer from scratch on a mixture of multimodal sequences.</p></div></div></div></div></div></section><section id=section-publications class=home-section><div class=container><div class=section-c-title>Publications</div><div><div style=padding-bottom:2rem>Full list in <a href="https://scholar.google.com/citations?user=R3_AR5EAAAAJ&hl=en">Google Scholar</a>.</div><ul><li><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<sapn class=pub-abbr>[CVPR 2025 <span>Oral</span> (Top 7‰)]</sapn>
<sapn class=pub-title>Video-XL: Towards Vision Language Models For Extra-Long Video Understanding.</sapn>
<sapn class=pub-author>Yan Shu, Zheng Liu, Peitian Zhang, Minghao Qin, Junjie Zhou, Zhengyang Liang, Tiejun Huang, Bo Zhao</sapn>
<sapn class=pub-extra_desc1>Oral Ratio: 96/13008</sapn><p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/pdf/2409.14485>PDF</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/VectorSpaceLab/Video-XL>Code</a></p></div></li><li><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<sapn class=pub-abbr>[CVPR 2025]</sapn>
<sapn class=pub-title>MLVU: Benchmarking Multi-task Long Video Understanding</sapn>
<sapn class=pub-author>Junjie Zhou*, Yan Shu*, Bo Zhao*, Boya Wu, Zhengyang Liang, Shitao Xiao, Minghao Qin, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, Zheng Liu</sapn>
<sapn class=pub-extra_desc1></sapn><p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/pdf/2406.04264>PDF</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/JUNJIE99/MLVU>Code</a></p></div></li><li><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<sapn class=pub-abbr>[CVPR 2025]</sapn>
<sapn class=pub-title>Seeing Clearly, Answering Incorrectly: A Multimodal Robustness Benchmark For Evaluating MLLMs On Leading Questions</sapn>
<sapn class=pub-author>Yexin Liu, Zhengyang Liang, Yueze Wang, Xianfeng Wu, Feilong Tang, Muyang He, Jian Li, Zheng Liu, Harry Yang, Ser-Nam Lim, Bo Zhao</sapn>
<sapn class=pub-extra_desc1></sapn><p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/pdf/2406.10638>PDF</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/BAAI-DCAI/MMVU>Code</a></p></div></li><li><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<sapn class=pub-abbr>[CVPR 2025 <span>Oral</span> (Top 7‰)]</sapn>
<sapn class=pub-title>Towards Universal Dataset Distillation via Task-Driven Diffusion</sapn>
<sapn class=pub-author>Ding Qi, Jian Li, Junyao Gao, Shuguang Dou, Ying Tai, Jianlong Hu, Bo Zhao, Yabiao Wang, Chengjie Wang, Cairong Zhao. Coming soon</sapn>
<sapn class=pub-extra_desc1>Oral Ratio: 96/13008</sapn></div></li><li><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<sapn class=pub-abbr>[ICRA 2025]</sapn>
<sapn class=pub-title>SpatialBot: Precise Spatial Understanding with Vision Language Models</sapn>
<sapn class=pub-author>Wenxiao Cai, Iaroslav Ponomarenko, Jianhao Yuan, Xiaoqi Li, Wankou Yang, Hao Dong, Bo Zhao</sapn>
<sapn class=pub-extra_desc1></sapn><p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/pdf/2406.13642>PDF</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/BAAI-DCAI/SpatialBot>Code</a></p></div></li><li><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<sapn class=pub-abbr>[ICRA 2025]</sapn>
<sapn class=pub-title>NaVid-4D: Unleashing Spatial Intelligence in Egocentric RGB-D Videos for Vision-and-Language Navigation</sapn>
<sapn class=pub-author>Haoran Liu* Weikang Wan*, Xiqian Yu*, Minghan Li*, Jiazhao Zhang, Bo Zhao, Zhibo Chen, Zhongyuan Wang, Zhizheng Zhang, He Wang</sapn>
<sapn class=pub-extra_desc1></sapn><p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://wkwan7.github.io/static/video/NaVid-4D.mp4>Video</a></p></div></li><li><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<sapn class=pub-abbr>[IJCV 2025]</sapn>
<sapn class=pub-title>Image Captions are Natural Prompts for Training Data Synthesis</sapn>
<sapn class=pub-author>Shiye Lei*, Hao Chen*, Sen Zhang, Bo Zhao†, Dacheng Tao†</sapn>
<sapn class=pub-extra_desc1>Coming Soon</sapn></div></li><li><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<sapn class=pub-abbr>[TIP 2024]</sapn>
<sapn class=pub-title>Normalizing Batch Normalization for Long-Tailed Recognition</sapn>
<sapn class=pub-author>Yuxiang Bao∗, Guoliang Kang∗, Linlin Yang, Xiaoyue Duan, Bo Zhao, Baochang Zhang</sapn>
<sapn class=pub-extra_desc1></sapn><p><a class="btn btn-outline-primary btn-page-header btn-sm" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10817528&amp;casa_token=i3GB15OegqsAAAAA:eZITSPfdetmf3kWWwJ1GFGBFoveg1f5Mmxcq_NPfVOUhZBihnRyyIArkp9NcyOxJ65BYIcfSX3w">PDF</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/yuxiangbao/NBN>Code</a></p></div></li><li><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<sapn class=pub-abbr>[NeurIPS 2024 <span>Spotlight</span> (Top 3%)]</sapn>
<sapn class=pub-title>SegVol: Universal and Interactive Volumetric Medical Image Segmentation</sapn>
<sapn class=pub-author>Yuxin Du, Fan Bai, Tiejun Huang, Bo Zhao</sapn>
<sapn class=pub-extra_desc1><span>The ninth-highest rated paper (9/15671) in NeurIPS 2024</span></sapn><p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://papercopilot.com/statistics/neurips-statistics/neurips-2024-statistics/>Ranking</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://openreview.net/pdf?id=105ZuvpdyW">PDF</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/BAAI-DCAI/SegVol>Code</a></p></div></li><li><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<sapn class=pub-abbr>[NeurIPS 2024]</sapn>
<sapn class=pub-title>Fetch and Forge: Efficient Dataset Condensation for Object Detection</sapn>
<sapn class=pub-author>Ding Qi, Jian Li, Jinlong Peng, Bo Zhao, Shuguang Dou, Jialin Li, Jiangning Zhang, Yabiao Wang, Chengjie Wang, Cairong Zhao</sapn>
<sapn class=pub-extra_desc1></sapn><p><a class="btn btn-outline-primary btn-page-header btn-sm" href="https://openreview.net/pdf?id=m8MElyzuwp">PDF</a></p></div></li><li><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<sapn class=pub-abbr>[NeurIPS 2024 D&amp;B Track]</sapn>
<sapn class=pub-title>Touchstone Benchmark: Are We on the Right Way for Evaluating AI Algorithms for Medical Segmentation</sapn>
<sapn class=pub-author>Pedro R. A. S. Bassi*, Wenxuan Li*, et al</sapn>
<sapn class=pub-extra_desc1></sapn><p><a class="btn btn-outline-primary btn-page-header btn-sm" href="https://openreview.net/pdf?id=YzM10FEJ2D">PDF</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/MrGiovanni/Touchstone>Code</a></p></div></li><li><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<sapn class=pub-abbr>[ECCV 2024]</sapn>
<sapn class=pub-title>Omni6DPose: A Benchmark and Model for Universal 6D Object Pose Estimation and Tracking</sapn>
<sapn class=pub-author>Jiyao Zhang*, Weiyao Huang*, Bo Peng*, Mingdong Wu, Fei Hu, Zijian Chen, Bo Zhao, Hao Dong</sapn>
<sapn class=pub-extra_desc1></sapn><p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://jiyao06.github.io/Omni6DPose/>Project Page</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/09574.pdf>PDF</a></p></div></li><li><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<sapn class=pub-abbr>[ACL 2024]</sapn>
<sapn class=pub-title>VISTA: Visualized Text Embedding For Universal Multi-Modal Retrieval</sapn>
<sapn class=pub-author>Junjie Zhou, Shitao Xiao, Zheng Liu, Bo Zhao, Yongping Xiong</sapn>
<sapn class=pub-extra_desc1></sapn><p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://aclanthology.org/2024.acl-long.175.pdf>PDF</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/visual>Code</a></p></div></li><li><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<sapn class=pub-abbr>[RSS 2024]</sapn>
<sapn class=pub-title>RAG-Driver Generalisable Driving Explanations with Retrieval-Augmented In-Context Learning in Multi-Modal Large Language Model</sapn>
<sapn class=pub-author>Jianhao Yuan, Shuyang Sun, Daniel Omeiza, Bo Zhao, Paul Newman, Lars Kunze, Matthew Gadd</sapn>
<sapn class=pub-extra_desc1></sapn><p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://yuanjianhao508.github.io/RAG-Driver/>Project Page</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/pdf/2402.10828>PDF</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/YuanJianhao508/RAG-Driver>Code</a></p></div></li><li><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<sapn class=pub-abbr>[ICLR 2024]</sapn>
<sapn class=pub-title>Real-Fake: Effective Training Data Synthesis Through Distribution Matching</sapn>
<sapn class=pub-author>Jianhao Yuan, Jie Zhang, Shuyang Sun, Philip Torr, Bo Zhao</sapn>
<sapn class=pub-extra_desc1></sapn><p><a class="btn btn-outline-primary btn-page-header btn-sm" href="https://openreview.net/pdf?id=svIdLLZpsA">PDF</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/BAAI-DCAI/Training-Data-Synthesis>Code</a></p></div></li><li><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<sapn class=pub-abbr>[CVPR 2023 <span>Highlight</span> (Top 3%)]</sapn>
<sapn class=pub-title>Accelerating Dataset Distillation via Model Augmentation</sapn>
<sapn class=pub-author>Lei Zhang*, Jie Zhang*, Bowen Lei, Subhabrata Mukherjee, Xiang Pan, Bo Zhao, Caiwen Ding, Yao Li, Dongkuan Xu</sapn>
<sapn class=pub-extra_desc1></sapn><p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Accelerating_Dataset_Distillation_via_Model_Augmentation_CVPR_2023_paper.pdf>PDF</a></p></div></li><li><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<sapn class=pub-abbr>[WACV 2023]</sapn>
<sapn class=pub-title>Dataset Condensation with Distribution Matching</sapn>
<sapn class=pub-author>Bo Zhao; Hakan Bilen</sapn>
<sapn class=pub-extra_desc1></sapn><p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/pdf/2110.04181.pdf>PDF</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/VICO-UoE/DatasetCondensation>Code</a></p></div></li><li><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<sapn class=pub-abbr>[NeurIPS Workshops 2022]</sapn>
<sapn class=pub-title>Synthesizing Informative Training Samples with GAN</sapn>
<sapn class=pub-author>Bo Zhao; Hakan Bilen</sapn>
<sapn class=pub-extra_desc1></sapn><p><a class="btn btn-outline-primary btn-page-header btn-sm" href="https://openreview.net/pdf?id=frAv0jtUMfS">PDF</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/VICO-UoE/IT-GAN>Code</a></p></div></li><li><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<sapn class=pub-abbr>[ICML 2022 <span>Outstanding Paper Award</span> (Top 2‰)]</sapn>
<sapn class=pub-title>Privacy for Free: How does Dataset Condensation Help Privacy</sapn>
<sapn class=pub-author>Tian Dong; Bo Zhao; Lingjuan Lyu</sapn>
<sapn class=pub-extra_desc1></sapn><p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://proceedings.mlr.press/v162/dong22c/dong22c.pdf>PDF</a></p></div></li><li><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<sapn class=pub-abbr>[CVPR 2022]</sapn>
<sapn class=pub-title>CAFE: Learning to Condense Dataset by Aligning Features</sapn>
<sapn class=pub-author>Kai Wang*; Bo Zhao*; Xiangyu Peng; Zheng Zhu; Shuo Yang; Shuo Wang; Guan Huang; Hakan Bilen; Xinchao Wang; and Yang You</sapn>
<sapn class=pub-extra_desc1></sapn><p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_CAFE_Learning_To_Condense_Dataset_by_Aligning_Features_CVPR_2022_paper.pdf>PDF</a></p></div></li><li><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<sapn class=pub-abbr>[ICML 2021]</sapn>
<sapn class=pub-title>Dataset Condensation with Differentiable Siamese Augmentation</sapn>
<sapn class=pub-author>Bo Zhao; Hakan Bilen</sapn>
<sapn class=pub-extra_desc1></sapn><p><a class="btn btn-outline-primary btn-page-header btn-sm" href=http://proceedings.mlr.press/v139/zhao21a/zhao21a.pdf>PDF</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/VICO-UoE/DatasetCondensation>Code</a></p></div></li><li><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<sapn class=pub-abbr>[ICLR 2021 <span>Oral</span> (Top 2%)]</sapn>
<sapn class=pub-title>Dataset Condensation with Gradient Matching</sapn>
<sapn class=pub-author>Bo Zhao; Konda Reddy Mopuri; Hakan Bilen</sapn>
<sapn class=pub-extra_desc1><span>The second-highest rated paper 2/2997 in ICLR 2021</span></sapn><p><a class="btn btn-outline-primary btn-page-header btn-sm" href="https://docs.google.com/spreadsheets/d/1n58O0lgGI5kI0QQY9f4BDDpNB4oFjb5D51yMr9fHAK4/edit#gid=1546418007">Ranking</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://openreview.net/pdf?id=mSAKhLYLSsl">PDF</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/VICO-UoE/DatasetCondensation>Code</a></p></div></li><li><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<sapn class=pub-abbr>[WACV 2021]</sapn>
<sapn class=pub-title>Continual Representation Learning for Biometric Identification</sapn>
<sapn class=pub-author>Bo Zhao*; Shixiang Tang*; Dapeng Chen; Hakan Bilen; Rui Zhao</sapn>
<sapn class=pub-extra_desc1></sapn><p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://openaccess.thecvf.com/content/WACV2021/papers/Zhao_Continual_Representation_Learning_for_Biometric_Identification_WACV_2021_paper.pdf>PDF</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/PatrickZH/Continual-Representation-Learning-for-Biometric-Identification>Code</a></p></div></li><li><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<sapn class=pub-abbr>[arXiv 2020]</sapn>
<sapn class=pub-title>iDLG: Improved Deep Leakage from Gradients</sapn>
<sapn class=pub-author>Bo Zhao; Konda Reddy Mopuri; Hakan Bilen</sapn>
<sapn class=pub-extra_desc1></sapn><p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/pdf/2001.02610.pdf>arXiv</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/PatrickZH/Improved-Deep-Leakage-from-Gradients>Code</a></p></div></li><li><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<sapn class=pub-abbr>[ICML 2018]</sapn>
<sapn class=pub-title>MSplit LBI: Realizing Feature Selection and Dense Estimation Simultaneously in Few-shot and Zero-shot Learning</sapn>
<sapn class=pub-author>Bo Zhao*; Xinwei Sun*; Yanwei Fu; Yuan Yao; Yizhou Wang</sapn>
<sapn class=pub-extra_desc1></sapn><p><a class="btn btn-outline-primary btn-page-header btn-sm" href="https://drive.google.com/open?id=1S8YvK9TpFXIFU-o6uZK8mmYRWGe-4rWX">PDF</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=http://github.com/PatrickZH/MSplitLBI>Code</a></p></div></li><li><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<sapn class=pub-abbr>[ACM TOG 2018 & SIGGRAPH 2019]</sapn>
<sapn class=pub-title>EasyFont: A Style Learning based System to Easily Build Your Large-scale Handwriting Fonts</sapn>
<sapn class=pub-author>Zhouhui Lian; Bo Zhao; Xudong Chen; Jianguo Xiao</sapn>
<sapn class=pub-extra_desc1></sapn><p><a class="btn btn-outline-primary btn-page-header btn-sm" href="https://drive.google.com/open?id=138PvyYdcIAcSDGEK3Qzp9pnQnFwbtwVs">PDF</a></p></div></li><li><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<sapn class=pub-abbr>[SIGGRAPH ASIA 2016]</sapn>
<sapn class=pub-title>Automatic Generation of Large-scale Handwriting Fonts via Style Learning</sapn>
<sapn class=pub-author>Zhouhui Lian; Bo Zhao; Jianguo Xiao</sapn>
<sapn class=pub-extra_desc1></sapn><p><a class="btn btn-outline-primary btn-page-header btn-sm" href="https://drive.google.com/open?id=15y2pqi1oxZ7wTBDo-lVkWG2ghyu5q5RV">PDF</a></p></div></li></ul></div></div></section><section id=section-members class=home-section><div class=container><div class=section-c-title>Members</div><div class="grid grid-cols-1 gap-x-6 gap-y-16 lg:grid-cols-6"><div><div class=card><img src=/mint-sjtu/media/members/bo-zhao.jpg alt><div class=card-body><div class=card-text>Bo Zhao</div><div class="card-text m-role">Associate Professor</div></div></div></div><div><div class=card><img src=/mint-sjtu/media/members/tmp-member.png alt><div class=card-body><div class=card-text>Hongming Fu</div><div class="card-text m-role">PhD Student</div><div class=card-text>(2025-)</div></div></div></div><div><div class=card><img src=/mint-sjtu/media/members/tmp-member.png alt><div class=card-body><div class=card-text>Haowen Hou</div><div class="card-text m-role">PhD Student</div><div class=card-text>(2025-)</div></div></div></div><div><div class=card><img src=/mint-sjtu/media/members/tmp-member.png alt><div class=card-body><div class=card-text>Yilei Lyu</div><div class="card-text m-role">Master Student</div><div class=card-text>(2024-)</div></div></div></div><div><div class=card><img src=/mint-sjtu/media/members/tmp-member.png alt><div class=card-body><div class=card-text>Yang Tian</div><div class="card-text m-role">Master Student</div><div class=card-text>(2025-)</div></div></div></div><div><div class=card><img src=/mint-sjtu/media/members/tmp-member.png alt><div class=card-body><div class=card-text>Zhengpeng Shi</div><div class="card-text m-role">Master Student</div><div class=card-text>(2025-)</div></div></div></div></div></div></section></div><div class=page-footer><div class=container><footer class=site-footer><p class=powered-by>Published with <a href="https://hugoblox.com/?utm_campaign=poweredby" target=_blank rel=noopener>Hugo Blox Builder</a> — the free, <a href=https://github.com/HugoBlox/hugo-blox-builder target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><script src=/mint-sjtu/js/vendor-bundle.min.5faf09821dbe513ca103e87bafd52766.js></script><script src=https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.js integrity crossorigin=anonymous></script><script id=search-hit-fuse-template type=text/x-template>
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script><script id=page-data type=application/json>{"use_headroom":false}</script><script src=/mint-sjtu/en/js/wowchemy.min.157265bafbf7bdc0dcfeb1b11322a8eb.js></script><script src=/mint-sjtu/js/wowchemy-map.a26e9d2f7238ba5b868384f1c5bc6477.js type=module></script></body></html>